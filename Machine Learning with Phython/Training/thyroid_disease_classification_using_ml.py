# -*- coding: utf-8 -*-
"""Thyroid_Disease_Classification_Using_ML.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1a27sV8ktNsZxOB3zZV3igzPZCqcq_Ex8

### Reading the data

In this step, data is uploaded on the notebook instance. we are gonna access to it and upload it to the Amazon S3.
"""

from sagemaker.sklearn.estimator import SKLearn
from sagemaker import get_execution_role
import sagemaker

prefix = 'Scikit-SVM-pipeline-thyroid-example'

bucket = sagemaker.Session().default_bucket()

WORK_DIRECTORY = 'thyroid_data'

sagemaker_session = sagemaker.Session()

train_input = sagemaker_session.upload_data(
    path='{}/{}'.format(WORK_DIRECTORY, 'train.csv'), 
    bucket=bucket,
    key_prefix='{}/{}'.format(prefix, 'train'))

"""- Creating StandardScalar model

In this step, the path to the script is defined and pass it as an entry point to the SKLearn. After configuraing the model we fit it with our input data
"""

sagemaker_session = sagemaker.Session()

script_path = 'sklearn_thyroid_featurizer.py'

sklearn_preprocessor = SKLearn(
    entry_point=script_path,
    role=get_execution_role(),
    train_instance_type="ml.m5.xlarge",
    sagemaker_session=sagemaker_session)
sklearn_preprocessor.fit({'train': train_input})

"""### Creating the SVM model"""

sagemaker_session = sagemaker.Session()

script_path = 'sklearn_thyroid_SVM.py'

sklearn_SVM_model = SKLearn(
    entry_point=script_path,
    role=get_execution_role(),
    train_instance_type="ml.m5.xlarge",
    sagemaker_session=sagemaker_session)

sklearn_SVM_model.fit({'train': train_input})

"""### Creatimg the pipline and deploy it to endpiont"""

from sagemaker.model import Model
from sagemaker.pipeline import PipelineModel

scikit_learn_inferencee_model = sklearn_preprocessor.create_model()
scikit_learn_SVM_model = sklearn_SVM_model.create_model()

model_name = 'inference-pipeline-SVM'
sm_model = PipelineModel(
    name=model_name, 
    role=get_execution_role(), 
    models=[
        scikit_learn_inferencee_model, 
        scikit_learn_SVM_model])

sm_model.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge', endpoint_name='thyroid-splitted-final')

"""# 2. Test the model

We deployed the test on the test dataset. The dataset is splitted and 70% is used for the training the model and 30% is used for the test model.
"""

import io
import pandas as pd
import json
import numpy as np
import boto3
from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score
from sklearn import metrics

data = pd.read_csv('./thyroid_data/test.csv')

runtime= boto3.client('runtime.sagemaker')

features = data.iloc[:, 0:-1]
target = data.iloc[:, -1]

X= np.array(features).astype('float32')
Y= np.array(target).astype('float32')

temp=[]

for x in X:
    response = runtime.invoke_endpoint(EndpointName='thyroid-splitted-final',
                                   ContentType='text/csv',
                                   Body=[",".join(x.astype(str))][0])

    result = json.loads(response['Body'].read().decode())
    test_pred = result['instances'][0]['features']
    temp.append(test_pred)

Y_predict =pd.Series(np.array(temp))

precison = precision_score(Y, Y_predict)
recall = recall_score(Y, Y_predict)
f1 = f1_score(Y, Y_predict)
accuracy = metrics.accuracy_score(Y, Y_predict)

print("accurecy: "+ str(accuracy))
print("precison: "+str(precison))
print("recall: "+ str(recall))
print("f1: "+str(f1))